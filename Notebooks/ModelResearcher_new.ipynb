{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "11d67855",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Andrey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Andrey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import pymorphy2\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import gensim.models\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "import zipfile\n",
    "import sys\n",
    "import requests, io\n",
    "import re \n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f7c41e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text:str, stop_words, punctuation_marks, morph):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    preprocessed_text = []\n",
    "    for token in tokens:\n",
    "        if token not in punctuation_marks:\n",
    "            lemma = morph.parse(token)[0].normal_form\n",
    "            if re.match(r'(\\d.|\\d)', lemma) is None:\n",
    "                if lemma not in stop_words:\n",
    "                    preprocessed_text.append(lemma)\n",
    "    return preprocessed_text\n",
    "\n",
    "def read_json(path: str):\n",
    "    file = open(path)\n",
    "    data = json.load(file)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "punctuation_marks = ['!', ',', '(', ')', ';', ':', '-', '?', '.', '..', '...', \"\\\"\", \"/\", \"\\`\\`\", \"»\", \"«\" ]\n",
    "stop_words = stopwords.words(\"russian\")\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b855a9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelResearcher:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "    \n",
    "    def preprocess_and_save(self, data_df: pd.DataFrame, path, text_field='text') -> pd.DataFrame:\n",
    "        # for preprocessing dataset. Use it only in critical cases cause it's too slow on big datasets\n",
    "        data_df['preprocessed_' + text_field] = data_df.apply(lambda row: preprocess(row[text_field], punctuation_marks, stop_words, morph), axis=1)\n",
    "        data_df_preprocessed = data_df.copy()\n",
    "        data_df_preprocessed = data_df_preprocessed.drop(columns=[text_field], axis=1)\n",
    "        data_df_preprocessed.reset_index(drop=True, inplace=True)\n",
    "        if path is not None:\n",
    "            data_df_preprocessed.to_json(path)\n",
    "        return data_df_preprocessed\n",
    "    \n",
    "    def preprocess_and_save_pairs(self, data_df: pd.DataFrame, path, text_field_1, text_field_2) -> pd.DataFrame:\n",
    "        data_df['preprocessed_' + text_field_1] = data_df.apply(lambda row: preprocess(row[text_field_1], punctuation_marks, stop_words, morph), axis=1)\n",
    "        data_df['preprocessed_' + text_field_2] = data_df.apply(lambda row: preprocess(row[text_field_2], punctuation_marks, stop_words, morph), axis=1)\n",
    "        data_df_preprocessed = data_df.copy()\n",
    "        data_df_preprocessed = data_df_preprocessed.drop(columns=[text_field_1, text_field_2], axis=1)\n",
    "        data_df_preprocessed.reset_index(drop=True, inplace=True)\n",
    "        if path is not None:\n",
    "            data_df_preprocessed.to_json(path)\n",
    "        return data_df_preprocessed\n",
    "    \n",
    "    def train(self, data_df: pd.DataFrame, model=\"w2v\"):            \n",
    "        if model == \"w2v\":\n",
    "            train_part = data_df['preprocessed_texts']\n",
    "            self.model = gensim.models.Word2Vec(sentences=train_part, min_count=5, vector_size=50, epochs=5)\n",
    "        elif model == \"fast_text\":\n",
    "            print(\"fast_text\")\n",
    "            train_part = data_df['preprocessed_texts'].tolist()\n",
    "            self.model = gensim.models.FastText(vector_size=50, min_count=5)\n",
    "            self.model.build_vocab(corpus_iterable=train_part)\n",
    "            self.model.train(corpus_iterable=train_part, total_examples=len(train_part), epochs=5)\n",
    "        return\n",
    "    \n",
    "    def predict_sentences_similarity(self, sentences_1: pd.Series, sentences_2: pd.Series):\n",
    "        if sentences_1.size != sentences_2.size:\n",
    "            return None\n",
    "        else:\n",
    "            if self.model is not None:\n",
    "                sentences_sim = np.zeros(sentences_1.size)\n",
    "                sz = sentences_1.size\n",
    "#                 print(f'index_to_key: {self.model.wv.index_to_key}')\n",
    "                for i in range(sz): \n",
    "                    sentences_1_words = [w for w in sentences_1[i] if w in self.model.wv.index_to_key]\n",
    "                    sentences_2_words = [w for w in sentences_2[i] if w in self.model.wv.index_to_key]\n",
    "                    sim = self.model.wv.n_similarity(sentences_1_words, sentences_2_words)\n",
    "                    sentences_sim[i] = sim\n",
    "                \n",
    "                return sentences_sim\n",
    "            else:\n",
    "                return None\n",
    "            \n",
    "   \n",
    "        return round(float(2*TP / (2*TP + FP + FN)), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1f1921b4",
   "metadata": {},
   "outputs": [],
   "source": [
    " def calc_f1_score(sim, df, match_threshold):\n",
    "    (TP, FP, FN, TN) = (0, 0, 0, 0)\n",
    "    for i in range(sim.size):\n",
    "        if df['need_match'][i]:\n",
    "            if sim[i] >= match_threshold: \n",
    "                TP += 1\n",
    "            else:\n",
    "                FN += 1\n",
    "        else:\n",
    "            if sim[i] >= match_threshold: \n",
    "                FP += 1\n",
    "            else:\n",
    "                TN += 1\n",
    "       \n",
    "    return round(float(2*TP / (2*TP + FP + FN)), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386e7e07",
   "metadata": {},
   "source": [
    "## Обучим word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0eca54d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = read_json('./preprocessed_documents.json')\n",
    "modelResearcher_w2v = ModelResearcher()\n",
    "modelResearcher_w2v.train(data_df, model=\"w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "db930123",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_unmatch_df = read_json('./dataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b19a5164",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_f1 = pd.concat([pd.DataFrame(match_unmatch_df[0:17]), pd.DataFrame(match_unmatch_df[30:])], axis=0)\n",
    "df_test_f1 = pd.DataFrame(match_unmatch_df[17:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "0c8edc15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train: 33 of 46\n",
      "df_test: 13 of 46\n"
     ]
    }
   ],
   "source": [
    "print('df_train: {} of {}'.format(df_train_f1['id_rp'].size, match_unmatch_df ['id_rp'].size))\n",
    "print('df_test: {} of {}'.format(df_test_f1['id_rp'].size,match_unmatch_df['id_rp'].size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "44c4ecf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_f1 = modelResearcher_w2v.preprocess_and_save_pairs(df_train_f1, None, 'text_rp', 'text_proj')\n",
    "df_test_f1 = modelResearcher_w2v.preprocess_and_save_pairs(df_test_f1, None, 'text_rp', 'text_proj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "bf5294fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_w2v_train = modelResearcher_w2v.predict_sentences_similarity(df_train_f1['preprocessed_text_rp'], df_train_f1['preprocessed_text_proj'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "bc1a5688",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score train w2v: 0.733\n"
     ]
    }
   ],
   "source": [
    "f1_w2v_train = calc_f1_score(result_w2v_train, df_train_f1, 0.78)\n",
    "print('F1-score train w2v: {}'.format(f1_w2v_train ))\n",
    "df_train_f1.drop('score', inplace=True, axis=1, errors='ignore')\n",
    "df_train_f1.insert(loc=4, column='score', value=result_w2v) \n",
    "# df_train_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "8fd4a226",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_w2v_test = modelResearcher_w2v.predict_sentences_similarity(df_test_f1['preprocessed_text_rp'], df_test_f1['preprocessed_text_proj'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ced54aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score test w2v: 0.727\n"
     ]
    }
   ],
   "source": [
    "f1_w2v_test = calc_f1_score(result_w2v_test, df_test_f1, 0.78)\n",
    "print('F1-score test w2v: {}'.format(f1_w2v_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "79fb69d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_f1.drop('score', inplace=True, axis=1, errors='ignore')\n",
    "df_test_f1.insert(loc=4, column='score', value=result_w2v_test) \n",
    "# df_test_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801b816b",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "5e97964f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelResearcher_ft = ModelResearcher()\n",
    "data_df = read_json('./preprocessed_documents.json')\n",
    "modelResearcher_ft.train(data_df, model=\"fast_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "48e4f38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score train ft: 0.667\n",
      "F1-score test ft: 0.833\n"
     ]
    }
   ],
   "source": [
    "result_ft_train = modelResearcher_ft.predict_sentences_similarity(df_train_f1['preprocessed_text_rp'], df_train_f1['preprocessed_text_proj'])\n",
    "f1_ft_train = calc_f1_score(result_ft_train, df_train_f1, 0.78)\n",
    "print('F1-score train ft: {}'.format(f1_ft_train ))\n",
    "\n",
    "result_ft_test = modelResearcher_ft.predict_sentences_similarity(df_test_f1['preprocessed_text_rp'], df_test_f1['preprocessed_text_proj'])\n",
    "f1_ft_test = calc_f1_score(result_ft_test, df_test_f1, 0.78)\n",
    "print('F1-score test ft: {}'.format(f1_ft_test ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
